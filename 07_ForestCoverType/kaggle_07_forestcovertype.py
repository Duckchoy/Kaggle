# -*- coding: utf-8 -*-
"""Kaggle_07_ForestCoverType.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dqlBfYn9sr43L6EXkyTl-urcpBDLOF5A

## Forest Cover Types (Kaggle)

This notebook discusses my solution for the "Forest Cover Type" problem on Kaggle. It is a *supervised*, multi-category (of forest types) *classification* problem.

The first part of obtaining the solution is just understanding and processing the data before jumping into model building. For that we proceed as follows:
- Making Sense of the Data
    - Baseline prediction before any data processing
    - Understanding the range of the data
    - Detecting and cleaning "null" values
    - Detecting and clearning outliers 

- Feature Analysis
    - Correlating features with target
    - Selecting, engineering features
        - Features with unique values
        - Co-linearity detection

- Separate training, target, validation

Now we turn to model building and performance evaluation.

- Dummy Classifier (Baseline accuracy)
- Compare Several Machine Learning Models
- Perform Hyperparameter Tuning on the Best Model
- Interpret Model Results
- Evaluate the Best Model with Test Data (replying the initiating question)
- Summary & Conclusions

Before discussing the solution we first import the dataset from Kaggle into the Google Drive. One can alo manually upload the data. All the useful libraries are also imported here.

### 0. Importing libraries, the data and utilities

#### 0.1 Imports
"""

# Libraries
from google.colab import drive

import os
from time import time 
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# machine learning models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingRegressor
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# preprocessing functions and evaluation models
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix
from sklearn.dummy import DummyClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# Mount your Google Drive
drive.mount('/gdrive', force_remount=True)

# Set the configuration (.jason) file to your working directory (here, "Kaggle")
os.environ['KAGGLE_CONFIG_DIR'] = "/gdrive/My Drive/Kaggle"

# Commented out IPython magic to ensure Python compatibility.
# changing the working directory

# %cd /gdrive/My Drive/Kaggle

# Uncomment the lines below to unzip the downloaded files

# !kaggle competitions download -c forest-cover-type-prediction
# !unzip \*.zip  && rm *.zip

"""#### 0.2 Utils"""

def one_hot_decode(dataframe, start_loc, end_loc, numeric_column_name):
    ''' 
    This function takes the start and end location of the one-hot-encoded column 
    set and numeric column name to be created as arguments
    (1) transforms one-hot-encoded columns into one column consisting of column 
        names with string data type
    (2) splits string column into the alphabetical and numerical characters
    (3) fetches numerical character and creates numeric column in the given dataframe
    '''

    def split_numbers_chars(row):
        '''
        This function fetches the numerical characters at the end of a string
        and returns alphabetical character and numerical chaarcters respectively
        '''
        head = row.rstrip('0123456789')
        tail = row[len(head):]
        return head, tail
    
    dataframe['String_Column'] = (dataframe.iloc[:, start_loc:end_loc] == 1
                                  ).idxmax(axis=1)
    dataframe['Tuple_Column'] = dataframe['String_Column'].apply(
                                                            split_numbers_chars)
    dataframe[numeric_column_name] = dataframe['Tuple_Column'].apply(
                                                lambda x: x[1]).astype('int64')

    dataframe.drop(columns=['String_Column','Tuple_Column'], inplace=True)

# function to train a given model, generate predictions, and return accuracy score
def fit_evaluate_model(model, X_train, y_train, X_valid, Y_valid):
    
    model.fit(X_train, y_train)
    
    y_train_pred = model.predict(X_train)
    y_valid_pred = model.predict(X_valid)

    train_accuracy = accuracy_score(y_train, y_train_pred)
    valid_accuracy = accuracy_score(y_valid, y_valid_pred)
    
    return (train_accuracy, valid_accuracy)

# IMP: call 'fit' method of the model before using this.
# Obtains (& plots) top 'top' features with highest importance in the RFC.
def rfc_feature_mdi(model, dataframe, top=20):
    """
    "Feature importances are .. computed as the mean and standard deviation of 
    accumulation of the impurity decrease within each tree" (sklearn)
    """
    importances = model.feature_importances_ 
    indices = np.argsort(importances)[::-1]    # reverse sort important features

    print(f"Top {top} Important Features\n")
    for i in range(top): 
        feature_name = dataframe.columns[indices[i]]
        feature_mdi = importances[indices[i]]   # mdi=mean decrease in impurity
        print(f"{i+1}. {feature_name} [{feature_mdi: 0.3} ]")
    print("\n")

    plt.bar(range(top), importances[indices[:top]])
    plt.title("Feature importances using MDI")
    plt.ylabel("Mean decrease in impurity (importance)")
    plt.xticks(range(top), dataframe.columns[indices], rotation=90)

"""### 1. Making sense of the data

In this section, we are going to the following things.
- "description" of the data [`pd.describe()`]
- Detecting "null" data [`pd.isnull()`]
"""

df_train = pd.read_csv("train.csv")
df_test = pd.read_csv("test.csv")

print("Training set =", df_train.shape)
print("Testinng set =", df_test.shape)

print("___The training set___")
df_train.head(10)
# df_train.tail(10)

"""The features above are:
* Elevation - Elevation in meters 
* Aspect - Aspect in degrees azimuth 
* Slope - Slope in degrees 
* Horizontal_Distance_To_Hydrology - Horizontal Dist to nearest surface water 
* Vertical_Distance_To_Hydrology - Vert. Dist to nearest surface water 
* Horizontal_Distance_To_Roadways - Horizontal Dist to nearest roadway 
* Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice 
* Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice 
* Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice 
* Horizontal_Distance_To_Fire_Points - Horizontal Dist to nearest wildfire ignition points 
* Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation 
* Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation 
* **Cover_Type** (7 types, integers 1 to 7) - Forest Cover Type designation
"""

# Descriptive statistics of the features.
df_train.describe()

# List all the features
features = df_test.columns.values.tolist()
features.remove("Id")

# There are some binary features and some numeral ones
numeral_features = features[:10]
binary_features = features[10:]

# Our target variable is the 'Cover_Type' (how many classes does it have?)
cover_types = sorted(df_train['Cover_Type'].unique())
print("Unique class labels=", cover_types)

"""These are, as stated in the competition, the following forest types:
1. Spruce/Fir
2. Lodgepole Pine
3. Ponderosa Pine
4. Cottonwood/Willow
5. Aspen
6. Douglas-fir
7. Krummholz 
"""

# Each cover type has the following value counts.
df_train['Cover_Type'].value_counts()
# Note that they are all the same (that's good) cause the trining algorithsm 
# get to lean about each class equally.

# Check if there are any null values
df_train.isnull().sum()

"""Great! There're no "null" or "NaN" values. Although this is a great way to catch missing values, if the data collector used a proxy (e.g. '0') for the missing value then we need to catch it in other ways (mostly using common sense). E.g., below we can see, 88 entries have 0 for Hillshade_3pm whereas the corresponding Hillshade_9am or Hillshade_Noon are all pretty "bright". Rainy days aside, this number seems a bit too high. Probably some of them are proxy for the missing values. We will discuss this later."""

df_train[df_train['Hillshade_3pm'] ==0]

# Notice that the 'Soil_Type' and the 'Wilderness_Area' are one-hot-encoded 
# features. There are 40 soil types and 4 wilderness areas. You can check they 
# are binary data

is_binary_columns = [column for column in df_train.columns if 
                     ("Wilderness" in column) | ("Soil" in column)]
pd.unique(df_train[is_binary_columns].values.ravel())

# Notice something interesting
print(df_train['Soil_Type7'].unique())
print(df_train['Soil_Type15'].unique())
# These two soil types are never present! (we'll get back to it later).

# Before moving forward lets just remove the 'Id' column as that is not a real 
# information but might screw our training.
df_train = df_train.drop(['Id'], axis=1)

"""*Section Summary & Action Plan*
* 4 different wilderness areas, 40 different soil types are already one-hot-encoded.
* There are 7 unique (equally distributed) cover types.
* There are no missing values but Hillside_3pm contains spurious zeros (must be regressed).
* Soil_Type 7 and 15 have no predictive function (must be removed)

---

### 2. Baseline Test
 So far we have done nothing to the raw data (except try to understand it). Let's just see how accurate a simple Random Forest classifier would be this "unpolished" dataset. Any data processing we do must improve from the baseline numbers, otherwise the processing is baseless.
"""

X_features = df_train.drop(['Cover_Type'], axis=1)
y_labels = df_train['Cover_Type'].values

X_train, X_valid, y_train, y_valid = train_test_split(
                                        X_features, 
                                        y_labels, 
                                        test_size=0.2, 
                                        random_state=42)

print('Training Data Shape:', X_train.shape)
print('Validation Data Shape:', X_valid.shape)

# The baseline classifier-we try a few basic methods and compare their performances.

# base_clf = SVC()  # Train: 64%, Valid: 63%
# base_clf = KNeighborsClassifier()   # Train: 88%, Valid: 81%
# base_clf = XGBClassifier()  # Train: 77%, Valid: 75%
base_clf = RandomForestClassifier() # Train: 100%, Valid: 87%

score_train, score_valid = fit_evaluate_model(base_clf, 
                            X_train, y_train, X_valid, y_valid)

score_train, score_valid = round(100*score_train,2), round(100*score_valid,2)

print(f"Training accuracy = {score_train}%")
print(f"Validation accuracy = {score_valid}%")

"""**Observation** 
- The plain vanila RFC already seems to be doing a great job compared to other methods. 
- Interestingly, a simple RFC is already proving to be 100% accurate on the training data! However, clearly, there is some degree of over-fitting as well. We will have to address this. We don't have much control over the data size so maybe our best bet is feature engineering. But before that lets see exactly which cover types are pulling down the validation accuracy (with the help of the confusion matrix).
"""

# Plot the confusion matrix
disp = plot_confusion_matrix(estimator=base_clf, 
                             X=X_valid, y_true=y_valid,
                             display_labels=cover_types,
                             cmap=plt.cm.Blues,
                             normalize='true')

disp.ax_.set_title("Normalized confusion matrix")

plt.show()

"""**Observation**: 
* Types 4 & 7 are most easily predictable. 
* Cover types 1-2 are being miss-labeled the most. In the EDA section we will try to understand the features that are most predictive of types 1-2 and try to engineering something with them.
"""

# So what features are the most important ones in the above prediction?
rfc_feature_mdi(model=base_clf, dataframe=df_train, top=15)

"""**Observation**: Clearly, 'Elevation' is a really important feature so engineering more features off of it might help. All the distances seem to be the second most important features so we try to play with them as well.

*Section Summary & Action Plan*
* A RFC works the best (validation accuracy=87%) on unprocessed data (tweak this model).
* There is high degree of variance (training accuracy=100%). (kill features)
* [4,7,5,6,3,1,2] is in order of increasing difficulty to predict the cover type. (find features)
* Elevation is the most important feature, followed by the distance. (feature engineer)

---

### 3. Exploratory Data Analysis (EDA)

#### 3.1 Outlier Detection
"""

# First plot the box-whisker plot to visualize the outliers

rows, cols = 2, 5
fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(24, 8))
start = 0
for j in range(rows):
    for i in range(cols):
        if start == len(numeral_features):
            break
        sns.boxplot(x=df_train[numeral_features[start]],
                    whis=3, ax=ax[j, i])
        # A large 'whis' value is used (default=1.5). Otherwise, there are way 
        # too many data-points outside the box.
        start += 1

# Now removing the outlier using Tukey's fences
# [Q1 - whis*iqr, Q3 + whis*iqr]  (iqr=Q3-Q1)

outlier_index = dict()

print("___Outlier Count per Feature___")

for feature in numeral_features:
    
    q3 = np.percentile(df_train[feature], 75)
    q1 = np.percentile(df_train[feature], 25)
    
    iqr = q3 - q1
    whis = 3
    
    l_bound = q1 - iqr * whis
    u_bound = q3 + iqr * whis
    
    # List of indices where the attribute value lies outside the bounds
    feature_outliers = df_train[(df_train[feature] < l_bound) | 
                                (df_train[feature] > u_bound)].index
    
    print(len(feature_outliers), ':', feature)

    outlier_index[feature] = feature_outliers
    # The outliers (row indices) are saved in a dictionary of features

"""**Observation**: Hillshades are not truely outliers beacause they are just RGB values (0 through 255). Therefore, we ignore the correspong outliers obtained above. The other important features with outliers are the distances. Looking at the range of these data, since fire point distance has the widest range we will definitely take its outlier seriously. The second important outlier would be rodways distance. Being so few outliers, we ignore them. The remaining features do not seem to have a sizable range (compared to the fire point distances) to be worth removing (at the cost of loosing about 100 data points). """

_, ax = plt.subplots(1, 2, figsize=(12, 4))

# Box plot before removing the outliers
sns.boxplot(x=df_train['Horizontal_Distance_To_Fire_Points'], whis=3, ax=ax[0])

df_train_updated = df_train.drop(outlier_index['Horizontal_Distance_To_Fire_Points'])

# Box plot after removing the outliers
sns.boxplot(x=df_train_updated['Horizontal_Distance_To_Fire_Points'], whis=3, ax=ax[1])
print("LEFT \u27F6 Removed ", df_train.shape[0] - df_train_updated.shape[0], "data points (rows) \u27F6 RIGHT \n")

"""We will study each category of features in more detail.

- Correlating features with Target

- Statistical distribution of features
    - Categorical data using box plots
    - Numeral data using hist plots

#### 3.2 Categorical Featues

##### 3.2.1 Wilderness Type
"""

# Create one column as Wilderness_Area_Type and represent it as categorical data
wild_1 = df_train_updated.columns.get_loc('Wilderness_Area1')
wild_4 = df_train_updated.columns.get_loc('Wilderness_Area4') + 1

df_train_updated['Wilderness_Area_Type'] = (df_train_updated.iloc[:, 
                                                wild_1:wild_4] == 1).idxmax(1)

# list of wilderness areas
wilderness_areas = sorted(df_train_updated['Wilderness_Area_Type'].value_counts().index.tolist())

# plot cover_type distribution for each wilderness area
for area in wilderness_areas:
    subset = df_train_updated[df_train_updated['Wilderness_Area_Type'] == area]
    sns.kdeplot(subset["Cover_Type"], label=area, linewidth=2)

# set title, legends and labels
plt.ylabel("Probability Density")
plt.xlabel("Cover_Type")
plt.legend()
plt.title("Density of Cover Types Among Different Wilderness Areas", size=14)
plt.show()

"""**Observation**: Area 4 has Type 3,4,6 forests the most. Area 3 has almost all types of forests except Type 4. Area 2 has mostly Type 1 and 7. Area 1 has 1, 5, 7 as the most prominent type. An important takeaway is that types 1, 2 & 7 all belong to area 1, 2, 3 with almost equal probabilities. So Wilderness_Area is not a strong predictor for imporving our predictibility for area 1 & 2.

##### 3.2.2 Soil Type
"""

# Lets first remove the type 7 & 15 as concluded before

try:
    df_train_updated = df_train_updated.drop(['Soil_Type7', 'Soil_Type15'], axis=1)
except KeyError:
    pass
    
# The removal has been performed and the number of colums must be fewer now.
df_train_updated.shape

# We also create a new column which removes the one-hot-encoding.
soil_1 = df_train_updated.columns.get_loc('Soil_Type1')
soil_40 = df_train_updated.columns.get_loc('Soil_Type40') + 1

one_hot_decode(df_train_updated, soil_1, soil_40, "Soil_Type")

# plot relationship of soil type and cover type among different wilderness areas
g = sns.FacetGrid(df_train_updated, col="Wilderness_Area_Type", 
                  col_wrap=2, height=5, col_order=wilderness_areas)

g = g.map(plt.scatter,"Cover_Type", "Soil_Type", edgecolor="w", color="g")

"""**Observation**: One thing we learn from this plot is, wilderness area aside, type 5,7 are very selective of the soil type. On the otherhand, type 1,2 seem to be doing fairly well in almost all kinds of soil types.

#### 3.3 Numeral Features

##### 3.3.1 Distributions
"""

# First plot the distribution functions of each consitnuous feature
rows, cols = 2, 5
fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 10))
start = 0
for i in range(rows):
    for j in range(cols):
        if start == len(numeral_features):
            break
        sns.histplot(df_train[numeral_features[start]], 
                     kde=True, bins=30, ax=ax[i,j])
        start += 1

"""**Observations**: 
- Hillshade 9am and Noon have left-skewed distributions.
- Horizontal_Distance_To_Firepoints, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Hydrology have right-skewed distributions.
- Elevation somewhat resembles a uniform distribution (or a tri-modal one?).
- Slope, Vertical_Distance_To_Hydrology, Hillshade_3pm show a more symmetric distribution.
- Notice the tall bar at 0 tick of Hillshade_3pm (recall missing value discussion earlier)

##### 3.3.2 Colinearity
"""

# make a list of numeric features and create a dataframe with them
all_features = numeral_features + wilderness_areas + ["Soil_Type"] + ["Cover_Type"]
trans_df = df_train_updated[all_features]
trans_df

# pearson coefficients with numeric soil type column
correlations = pd.DataFrame(trans_df.corr())

# plot the heatmap
colormap = plt.cm.RdBu
fig, ax = plt.subplots(figsize=(24,12)) 
sns.heatmap(correlations,linewidths=0.1, 
            square=False, cmap=colormap, linecolor='white', annot=True)
plt.title('Pearson Correlation of Features (Added "Soil_Type" numerical feature', size=16)
plt.show()

"""**Observation** There are clearly some strong colinearities and we might want to address them. First we visualize what exactly their correlation looks like. The following correlations are analyzed:
1. Hillshade_3pm -- Hillshade_9am (r = -78%)
2. Hillshade_3pm -- Hillshade_Noon (r = 61%)
3. Vertical_ -- Horizontal_Distance_To_Hydrology (r = 65%)
Let's have a better look at them

"""

fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24,8)) 

sns.scatterplot(x="Vertical_Distance_To_Hydrology", y="Horizontal_Distance_To_Hydrology", 
                hue="Cover_Type", data=df_train_updated, 
                legend="full", hue_norm=(0,8), palette="Set1", ax=ax[0])

sns.scatterplot(x="Hillshade_Noon", y="Hillshade_3pm", 
                hue="Cover_Type", data=df_train_updated, 
                legend="full", hue_norm=(0,8), palette="Set1", ax=ax[1])

sns.scatterplot(x="Hillshade_9am", y="Hillshade_3pm", 
                hue="Cover_Type", data=df_train_updated, 
                legend="full", hue_norm=(0,8), palette="Set1", ax=ax[2])

"""The proxy zeros show up again as the horizontal line in the above figures. We clearly need to remove them. To do so we check hillshade at 9am and noon (when 3pm has zeros) and from there we regress 3pm data.

##### 3.3.3. Hillshade 3pm [work from here]

[this git](https://shankarmsy.github.io/posts/forest-cover-types.html)

[check against](https://github.com/cereniyim/Tree-Classification-ML-Model/blob/master/fantastic-trees-where-to-find-how-to-detect-them.ipynb)
"""

smaller_frame=trans_df[trans_df['Hillshade_3pm'] ==0]

plt.scatter(smaller_frame.Hillshade_9am,smaller_frame.Hillshade_Noon, 
            c=smaller_frame.Cover_Type, s=50, cmap=plt.cm.Oranges) 
plt.title('Correlation 9am-Noon for Hillshade_3pm=0', fontsize=15) 
plt.xlabel('Hillshade_9am') 
plt.ylabel('Hillshade_Noon') 
plt.show()

"""### 4. Feature Engineering"""

# Use Euclidean distance
euclid = (trans_df['Horizontal_Distance_To_Hydrology']**2 + 
          trans_df['Vertical_Distance_To_Hydrology']**2)**0.5
trans_df.loc[:, 'Euclidean_Distance_To_Hydrology'] = euclid
trans_df.loc[:, 'Sqrt_Euclidean_Distance_To_Hydrology'] = euclid**0.5

# effective elevation of a tree
eff_elev = (trans_df['Elevation'] + trans_df['Vertical_Distance_To_Hydrology'])/2
trans_df.loc[:, 'Mean_Elevation_Vertical_Distance_Hydrology'] = eff_elev
trans_df.loc[:, 'Sqrt_Mean_Elevation_Vertical_Distance_Hydrology'] = eff_elev**0.5

#
hydro_fire = (trans_df['Horizontal_Distance_To_Hydrology'] + 
     trans_df['Horizontal_Distance_To_Fire_Points'])/2
trans_df.loc[:, 'Mean_Distance_Hydrology_Firepoints'] = hydro_fire
trans_df.loc[:, 'Sqrt_Mean_Distance_Hydrology_Firepoints'] = hydro_fire**0.5

#
hydro_road = (trans_df['Horizontal_Distance_To_Hydrology'] + 
                trans_df['Horizontal_Distance_To_Roadways'])/2
trans_df['Mean_Distance_Hydrology_Roadways'] = hydro_road
trans_df['Sqrt_Mean_Distance_Hydrology_Roadways'] = hydro_road**0.5

# 
fire_road = (trans_df['Horizontal_Distance_To_Fire_Points'] + 
                trans_df['Horizontal_Distance_To_Roadways'])/2
trans_df.loc[:, 'Mean_Distance_Firepoints_Roadways'] = fire_road
trans_df.loc[:, 'Sqrt_Mean_Distance_Firepoints_Roadways'] = fire_road**0.5

trans_df.shape

# Let's take a look at the new correlation matrix
correlations = pd.DataFrame(trans_df.corr())

# plot the heatmap
colormap = plt.cm.RdBu
fig, ax = plt.subplots(figsize=(24,12)) 
sns.heatmap(correlations,linewidths=0.1, 
            square=False, cmap=colormap, linecolor='white', annot=True)
plt.show()

# You can see how each feature correlate with the target variable (Cover_Type).
# For clarity, let's print these values in an orderly fashion so that we can
# eliminate the features with the least correlations.
# create a df after droppping all the Soil_TypeX.

correlations_transformed = pd.DataFrame(trans_df.corr())
correlations_transformed = pd.DataFrame(correlations_transformed["Cover_Type"]).reset_index()

# format, and display sorted correlations_transformed
correlations_transformed.columns = ["Feature", "Correlation with Cover_Type"]
correlations_transformed = (correlations_transformed[correlations_transformed["Feature"] != "Cover_Type"]
                .sort_values(by="Correlation with Cover_Type", ascending=True, key=abs))
display(correlations_transformed)

# We are drawing a cut-off of corr~0.03. Features with lower correlation 
# are dropped from the final dataframe. This number is arbitrary, sort of 
# a hyperparameter. Note, keeping a large number of features will introduce
# high varaince. So play with this cut-off 

final_df = trans_df.drop([
                          'Euclidean_Distance_To_Hydrology', 
                          'Horizontal_Distance_To_Hydrology',
                          'Aspect',
                          'Hillshade_9am',
                          'Sqrt_Mean_Elevation_Vertical_Distance_Hydrology',
                          'Mean_Elevation_Vertical_Distance_Hydrology',
                          'Sqrt_Mean_Distance_Hydrology_Firepoints',
                          'Sqrt_Mean_Distance_Hydrology_Roadways',
                          'Sqrt_Mean_Distance_Firepoints_Roadways'
                          ], axis=1)

final_df

"""### 5. Split & Standardize"""

# We now separate the features and the targets from the dataframe
trees_training = final_df.drop(['Cover_Type'], axis=1)
labels_training = final_df["Cover_Type"].values

# Separate the data into training and validation sets

seed = 12
X_train, X_valid, y_train, y_valid = train_test_split(trees_training, labels_training, 
                                                      test_size=0.2, random_state=seed)

print('Training Data Shape:', X_train.shape)
print('Validation Data Shape:', X_valid.shape)

# create scaler
sc = StandardScaler()

# apply standardization and transform the training set
X_train_scaled = sc.fit_transform(X_train, y_train)

# transform validation set
X_valid_scaled = sc.transform(X_valid)

"""### 4. Model Testing

#### 4.1 Models out-of-the-box
We first test a few well known classifiers to see how they fare with each other, and most importantly with the baseline predictor.

##### 4.1.0 Dummy Predictor
"""

# Create dummy classifer
dummy = DummyClassifier(strategy='stratified', random_state=1)

# train the model
dummy.fit(X_train_scaled, y_train)

# Get accuracy score
baseline_accuracy = dummy.score(X_valid_scaled, y_valid)
print("Dummy algorithm classified {:0.2f} of the of the trees correctl!".format(baseline_accuracy))

"""##### 4.1.1 SVC"""

clf = SVC()
score_train, score_valid = fit_evaluate_model(clf, 
                            X_train_scaled, y_train, X_valid_scaled, y_valid)

score_train, score_valid = round(100*score_train,2), round(100*score_valid,2)

print(f"Training accuracy = {score_train}%")
print(f"Validation accuracy = {score_valid}%")

"""##### 4.1.2 K-NN Classifier"""

clf = KNeighborsClassifier()
score_train, score_valid = fit_evaluate_model(clf, 
                            X_train_scaled, y_train, X_valid_scaled, y_valid)

score_train, score_valid = round(100*score_train,2), round(100*score_valid,2)

print(f"Training accuracy = {score_train}%")
print(f"Validation accuracy = {score_valid}%")

"""##### 4.1.3 XGBoost"""

clf = XGBClassifier()
score_train, score_valid = fit_evaluate_model(clf, 
                            X_train_scaled, y_train, X_valid_scaled, y_valid)

score_train, score_valid = round(100*score_train,2), round(100*score_valid,2)

print(f"Training accuracy = {score_train}%")
print(f"Validation accuracy = {score_valid}%")

"""##### 4.1.4 Random Forest"""

clf = RandomForestClassifier(n_estimators=100, criterion="entropy")
score_train, score_valid = fit_evaluate_model(clf, 
                            X_train_scaled, y_train, X_valid_scaled, y_valid)

score_train, score_valid = round(100*score_train,2), round(100*score_valid,2)

print(f"Training accuracy = {score_train}%")
print(f"Validation accuracy = {score_valid}%")

"""Random Forest seems to perform the best. Clearly there is a lot of variance at this configuration though. Next we perform hyper-parameter optimization to reduce variance. Since it's Random Forest, we will use a meta estimator for this (ExtraTreesClassifier) that fits a number of randomized decision trees on various sub-samples of the dataset. We also usek-fold cross validation.

#### 4.2 Hyper-parameter Optimization
"""

# We are going to search over the hparam space given by the following vectors.

# The number of trees in the forest algorithm (default=100).
n_estimators = [50, 100, 300, 500, 700, 900]

# The function to measure the quality of a split (default='gini').
criterion = ['gini', 'entropy']

# The minimum number of samples required to split an internal node (default=2).
min_samples_split = [0.5, 2, 3, 5, 7]

# The number of features to consider when looking for the best split (default=auto).
max_features = ['auto', 'sqrt', 'log2', None] 

# Complexity parameter used for Minimal Cost-Complexity Pruning (default=2)
ccp_alpha = [0.0, 0.5, 1.0, 2.0]

# Define the grid of hyperparameters to search
hparam_grid = {'n_estimators': n_estimators,
              'criterion' : criterion,
              'min_samples_split': min_samples_split,
              'max_features': max_features,
              'ccp_alpha': ccp_alpha
              }

# create model
model = ExtraTreesClassifier(random_state=10)

# create Randomized search object
random_cv = RandomizedSearchCV(estimator=model,
                               param_distributions=hparam_grid,
                               cv=5,        # 5-fold cross validation
                               n_iter=20,   # Randomly try 10 different combinations of the hparams
                               scoring = 'accuracy',    # performance evaluation metric
                               verbose = 1, 
                               return_train_score = True, 
                               random_state=42)

# Fit on the all training data using random search object
random_cv.fit(trees_training, labels_training)

random_cv.best_estimator_

# Check accuracies of the above optimized classifier.
# max_features='log2' does slightly better..

optimal_clf = ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='entropy', max_depth=None, max_features='log2',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_impurity_split=None,
                     min_samples_leaf=1, min_samples_split=2,
                     min_weight_fraction_leaf=0.0, n_estimators=300,
                     n_jobs=None, oob_score=False, random_state=10, verbose=0,
                     warm_start=False)

score_train, score_valid = fit_evaluate_model(optimal_clf, 
                            X_train_scaled, y_train, X_valid_scaled, y_valid)

score_train, score_valid = round(100*score_train,2), round(100*score_valid,2)

print(f"Training accuracy = {score_train}%")
print(f"Validation accuracy = {score_valid}%")

"""#### 4.3 Model Visualization"""

disp = plot_confusion_matrix(estimator=optimal_clf, 
                             X=X_valid, 
                             y_true=y_valid,
                             display_labels=cover_types,
                             cmap=plt.cm.Blues,
                             normalize='true'
                             )

disp.ax_.set_title("Normalized confusion matrix")

plt.show()